import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
get_ipython().run_line_magic("matplotlib", " inline")
import pandas as pd
from sklearn.model_selection import train_test_split


#create a model class that inherits the NN module
class Model(nn.Module):
    #Input layer  (4 features of the flower) ---> hidden layer 1 (neurons)  ----> h2(neurons) ---> output
    def __init__(self,in_features=4,h1=8,h2=9,out_features=3):
        super().__init__() #instantinate our nn.module
        self.fc1 = nn.Linear(in_features,h1)
        self.fc2 = nn.Linear(h1,h2)
        self.out = nn.Linear(h2,out_features)
        
    def forward(self,x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.out(x)

        return x


#pick a manual seed for randomization
torch.manual_seed(32)

#create an instance of model
model =  Model()


url = 'https://gist.githubusercontent.com/netj/8836201/raw/6f9306ad21398ea43cba4f7d537619d0e07d5ae3/iris.csv'
df = pd.read_csv(url)
df.tail()


#change from string to integers
df['variety'] = df['variety'].replace('Setosa', 0)
df['variety'] = df['variety'].replace('Versicolor', 1)
df['variety'] = df['variety'].replace('Virginica', 2)
df


#Train test and split X
X = df.drop('variety',axis=1)
y = df['variety'] 


#convert to numpy array
X = np.array(X)
y = np.array(y)


#train test split
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2, random_state=41 ) 


#convert to float tensors
X_train = torch.FloatTensor(X_train)
X_test = torch.FloatTensor(X_test)


#convert to long tensors
y_train = torch.tensor(y_train,dtype=torch.long)
y_test = torch.tensor(y_test,dtype=torch.long)


#set the criterion of model to measure the error
criterion = nn.CrossEntropyLoss()

#choose Adam optimizer and lr = learning rate
optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)


model.parameters
#Train our models
#Epochs we want?
epochs = 100
losses = []
for i in range(epochs):
    #go forward and get a prediction
    y_pred = model.forward(X_train)


    #measure the loss/error 
    loss = criterion(y_pred,y_train)

    #keep track of our losses
    losses.append(loss.detach().numpy())

    #print every 10 epochs
    if i % 10 ==0:
        print(f'Epoch: {i} and loss: {loss}')

    #back propogation
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()


#graph the loss
plt.plot(range(epochs), losses)
plt.ylabel("Loss/Error")
plt.xlabel("Epochs")


#Evaluate model on test data (validate model on test)
with torch.no_grad(): #Basically turns off back propogation
    y_eval = model.forward(X_test) #X_test are features from test set and y_val will be our evaluation
    loss = criterion(y_eval,y_test)


loss


correct = 0
with torch.no_grad():
    for i,data in enumerate(X_test):
        y_val = model.forward(data)

        #will tell us what type does our network predicts
        print(f'{i+1}.) {str(y_val)} \t {y_test[i]} \t {y_val.argmax().item()}')

        #correct or not
        if y_val.argmax().item() == y_test[i]:
            correct+=1

print(f'We got {correct} correct out of 30')


#predict the class for a given dataset
new_iris = torch.tensor([4.7,3.2,1.3,0.2])


with torch.no_grad():
    print(model(new_iris))


#save or NN model
torch.save(model.state_dict(), 'my_first_NN_model.pt')


#load the saved model
new_model = Model()
new_model.load_state_dict(torch.load('my_first_NN_model.pt'))


#make sure it loaded correctly
new_model.parameters
